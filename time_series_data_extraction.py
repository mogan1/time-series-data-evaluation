# -*- coding: utf-8 -*-
"""time-series-data-extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S_eLqvd-QvVLLjEBGLT-ehnFlnI8TgaM

### Time Series - Data Extraction and cleansing
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""#### Import necessary packages"""

import matplotlib
import sklearn
import statsmodels as sm
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import matplotlib.dates as mdates
import numpy as np
from google.cloud import bigquery
import itertools
from collections import defaultdict
from copy import deepcopy
from typing import List
import json
import datetime as dt
import seaborn as sns
# pd.options.display.float_format = '{:.6f}'.format
from prophet import Prophet

"""##### Check Versions"""

print(f'''
pandas -> {pd.__version__}
numpy -> {np.__version__}
matplotlob -> {matplotlib.__version__}
statsmodels -> {sm.__version__}
scikit-learn -> {sklearn.__version__}
''')

path = 'drive/MyDrive/TimeSeries'

"""## Define generic functions

##### Load csv and index date attribute
"""

def read_dataset(csvfile, date_col=None):
    '''
    folder: is a Path object
    file: the CSV filename in that Path object.
    date_col: specify a date_col (optional)

    returns: a pandas DataFrame
    '''
    path = 'drive/MyDrive/TimeSeries'
    full_path = path+'/'+csvfile

    try:
        if date_col is not None:
            # If date_col is specified, parse dates and set the index
            df = pd.read_csv(full_path, parse_dates=[date_col])

        else:
            # If date_col is not specified, just load the CSV without parsing dates
            df = pd.read_csv(full_path)

        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
        return df
    except FileNotFoundError:
        raise FileNotFoundError(f"The file '{full_path}' was not found.")

"""##### Different evaluation metrics"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

def evaluate_forecast(method_name, predictions, actual_values):
    """
    Evaluate forecasting model performance using various metrics.

    Parameters:
    - method_name: Name of the forecasting method (e.g., 'Prophet', 'SARIMA','LSTM').
    - predictions: Predicted values from the forecasting model.
    - actual_values: Actual values from the test dataset.

    Returns:
    - Dictionary containing evaluation metrics with method name.
    """
    metrics = {'Method': method_name}

    # Mean Absolute Error (MAE)
    mae = mean_absolute_error(actual_values, predictions)
    metrics['MAE'] = mae

    # Mean Squared Error (MSE)
    mse = mean_squared_error(actual_values, predictions)
    metrics['MSE'] = mse

    # Root Mean Squared Error (RMSE)
    rmse_val = np.sqrt(mse)
    metrics['RMSE'] = rmse_val

    # Mean Absolute Percentage Error (MAPE)
    mape = np.mean(np.abs((actual_values - predictions) / actual_values)) * 100
    metrics['MAPE'] = mape

    return metrics

# Example usage:
# Assuming predictions and test_df['y'] are available
# method_name = 'Prophet'  # Change this based on the forecasting method
# metrics = evaluate_forecast(method_name, predictions, test_df['y'])
# print(metrics)

"""##### Split Train and test sets"""

def split_df_train_test(df_input, sort_col_date,split_percentage ):
  '''
    df_input: Input dataframe suitable for timeseries model
    sort_col: is date column that needs to be sorted
    split_percentage: specify a split percentage

    returns: two pandas DataFrame, train and test
    '''

  df_working = df_input.copy()

  # Convert 'ds' to datetime type

  df_working[sort_col_date] = pd.to_datetime(df_working[sort_col_date])
  df_working = df_working.sort_values(by=sort_col_date)
  df_working.set_index(sort_col_date, inplace=True)

  # Calculate the split index

  split_index = int(len(df_working) * split_percentage)

  # Split the DataFrame into training and test sets
  train_df = df_working[:split_index]
  test_df = df_working[split_index:]

  # Display the lengths of the training and test sets
  print("Training set length:", len(train_df))
  print("Test set length:", len(test_df))

  return train_df, test_df

"""#### Plot actual vs predicted"""

def plot_actual_vs_predicted(dataframe, actual_column='actual', predicted_column='predicted'):
    """
    Plot actual vs predicted values from an indexed DataFrame.

    Parameters:
    - dataframe: DataFrame indexed by date, containing 'actual_column' and 'predicted_column'.
    - actual_column: Name of the actual values column (default is 'actual').
    - predicted_column: Name of the predicted values column (default is 'predicted').

    Returns:
    - None (displays the plot).
    """
    plt.figure(figsize= (14, 5))
    plt.plot(dataframe.index, dataframe[actual_column], label='Actual', marker='o')
    plt.plot(dataframe.index, dataframe[predicted_column], label='Predicted', marker='o')

    # Adding labels and title
    plt.xlabel('Date')
    plt.ylabel('Value')
    plt.title('Actual vs Predicted')
    plt.legend()
    plt.grid(True)

    # Adjust layout
    plt.tight_layout()

    # Display the plot
    plt.show()

# Example usage:
# Assuming prophet_df is your indexed DataFrame
# plot_actual_vs_predicted_indexed(prophet_df)

"""## Step 1 : Load and prepare necessary data for the analysis

##### Post Code Group - cluster . This is used by business users. Not an unsupervised cluster
"""

df_cluster = read_dataset('cluster.csv')
df_cluster['post_code'] = df_cluster['post_code'].astype(str)

df_cluster.head(10)

"""##### Load delivery methods"""

df_delivery_services = read_dataset('delivery_services.csv')

df_delivery_services.head(10)

df_delivery_services.dtypes

"""##### Load timeseries data having 2 years worth of granular data at post code and service type"""

df_delivery_transactions=read_dataset('delivery_transactions.csv',date_col='delivery_date')

df_delivery_transactions['post_code'] = df_delivery_transactions['post_code'].astype(str)

df_delivery_transactions.head(10)

df_delivery_transactions.dtypes

"""##### Now we will create a denormalized dataset for adding more dimensions"""

df_delivery_transactions.sort_values(by='delivery_date', inplace=True)
#df_delivery_transactions.reset_index('delivery_date', inplace=True)
df_transaction_merge = pd.merge(df_delivery_transactions, df_cluster, on='post_code', how='left')
df_transaction_merge = pd.merge(df_transaction_merge, df_delivery_services, on='service_type_code', how='left')
df_transaction_merge.drop(['country_y'], axis=1, inplace=True)
df_transaction_merge.rename(columns={'country_x': 'country'}, inplace=True)

df_transaction_merge.head(5)

"""##### Aggregate the data at service level"""

df_transaction_agg = df_transaction_merge.groupby(['region', 'country', 'delivery_date',  'service_desc']).agg({'cnt_work_order': 'sum'}).reset_index()

df_transaction_agg.head(5)

"""##### Save the output to drive for reducing future reprocessing"""

df_transaction_agg.to_csv('/content/drive/MyDrive/TimeSeries/delivery_transactions_agg_by_services.csv')

df_transaction_agg = df_transaction_merge.groupby(['region', 'country', 'delivery_date',  'service_desc']).agg({'cnt_work_order': 'sum'}).reset_index()

"""## Step 2 :DQ Check and data imputation"""

df_transaction_agg.isnull().sum()

df_transaction_agg.info()

"""##### Different Services that are involved"""

unique_service_desc = df_transaction_agg['service_desc'].unique()

# Display unique service descriptions with serial numbers
for i, service_desc in enumerate(unique_service_desc, start=1):
    print(f"{i}. {service_desc}")

"""##### Create a dataset having major observations"""

# Count the number of unique delivery dates for each service description
date_counts = df_transaction_agg.groupby('service_desc')['delivery_date'].nunique().sort_values(ascending=False)

# Plotting
plt.figure(figsize=(12, 6))
sns.barplot(x=date_counts.index, y=date_counts.values, order=date_counts.index, palette='viridis')
plt.title('Count of Distinct Delivery Dates by Service Description')
plt.xlabel('Service Description')
plt.ylabel('Count of Distinct Delivery Dates')
plt.xticks(rotation=45, ha='right')  # Rotating x-axis labels for better visibility
plt.show()

# Group by 'service_desc' and count unique dates
date_counts = df_transaction_agg.groupby('service_desc')['delivery_date'].nunique()

# Filter service_desc based on the count condition
valid_service_desc = date_counts[date_counts >= 700].index

# Filter the original DataFrame based on the valid service_desc
df_transaction_comparable = df_transaction_agg[df_transaction_agg['service_desc'].isin(valid_service_desc)]

df_transaction_comparable.dtypes

df_transaction_comparable.groupby('service_desc')['delivery_date'].nunique()

df_transaction_comparable.dtypes

"""##### Identify is the timeseries has continous values otherwise we need impute with forward fill"""

# Step 1: Get unique service_desc values
unique_service_desc = df_transaction_comparable['service_desc'].unique()

# Step 2: Generate a list of all dates between '2021-09-01' and '2023-08-30'
date_range = pd.date_range(start='2021-09-01', end='2023-08-30')

# Step 3 and 4: Iterate over each combination of service_desc and date
new_rows = []
for service_desc in unique_service_desc:
    for date in date_range:
        # Check if a row with the combination exists
        if not ((df_transaction_comparable['service_desc'] == service_desc) & (df_transaction_comparable['delivery_date'] == date)).any():
            # If not, add a new row with forward-filled value for 'cnt_work_order'
            new_rows.append({
                'region': 'EU',
                'country': 'SE',
                'delivery_date': date,
                'service_desc': service_desc,
                'cnt_work_order': df_transaction_comparable.loc[
                    (df_transaction_comparable['service_desc'] == service_desc) &
                    (df_transaction_comparable['delivery_date'] < date), 'cnt_work_order'
                ].ffill().iloc[-1]  # Forward-fill and take the last value
            })

# Step 5: Concatenate the new rows to the original DataFrame
df_modified = pd.concat([df_transaction_comparable, pd.DataFrame(new_rows)], ignore_index=True)
df_modified = df_modified.sort_values(by='delivery_date')
# Reset the index after sorting
df_modified = df_modified.reset_index(drop=True)

df_modified.groupby('service_desc')['delivery_date'].nunique()

"""## Step 3 : Exploratory data analysis

##### Exploratory - Plot the graph for all services togeher
"""

# Create a Seaborn color palette
palette = sns.color_palette("tab20", n_colors=len(df_transaction_comparable['service_desc'].unique()))

# Plot with Seaborn
plt.figure(figsize=(24, 12))
sns.lineplot(data=df_transaction_comparable, x=df_transaction_comparable.index, y='cnt_work_order', hue='service_desc', palette=palette, ci=None)

# Customize the plot
plt.title('Service Description Comparison')
plt.xlabel('Delivery Date')
plt.ylabel('Count of Work Order')
plt.legend(title='Service Description', bbox_to_anchor=(1, 1), loc='upper left')

# Show the plot
plt.show()

"""##### Exploring timeseries profile for individual services"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("ticks")

# Set up the FacetGrid
g = sns.FacetGrid(
    df_transaction_comparable,
    col_wrap=2,
    col="service_desc",
    sharey=False,
    height=15,
    aspect=1.5,
    despine=True
)

# Map the lineplot to each subplot
g.map(sns.lineplot, "delivery_date", "cnt_work_order")

# Customize titles, axis labels, and tick parameters
g.set_titles(size=40)
g.set_axis_labels("Delivery Date", "Count of Work Order", fontsize=40)

for ax in g.axes.flat:
    ax.grid(True, axis='both')
    ax.tick_params(axis='x', labelrotation=45, labelsize=20)  # Adjust label size
    ax.tick_params(axis='y', labelrotation=45, labelsize=20)  # Adjust label size

# Adjust layout and spacing
plt.tight_layout()

# Show the plot
plt.show()

df_modified.to_csv('delivery_trasactions_service_balanced.csv', index=False)

"""##### Process: trend, seasonality, and residual"""

from statsmodels.tsa.seasonal import seasonal_decompose

df_modified['delivery_date'] = pd.to_datetime(df_modified['delivery_date'])

# Set up unique services
unique_filtered_service_desc = df_modified['service_desc'].unique()

# Create an empty DataFrame to store results
decomposition_results = pd.DataFrame()

# Set up FacetGrid
g = sns.FacetGrid(df_modified, col="service_desc", col_wrap=3, height=5, aspect=1.5)
g.map_dataframe(lambda data, **kwargs: sns.lineplot(x='delivery_date', y='cnt_work_order', data=data.copy(), **kwargs))
g.set_axis_labels("Delivery Date", "Count of Work Order")
g.set_titles(col_template="{col_name}", size=20)

# Customize each subplot
for service_desc in unique_filtered_service_desc:
    # Filter DataFrame for the specific service
    df_service = df_modified[df_modified['service_desc'] == service_desc].copy()

    # Set 'delivery_date' as the index
    df_service.set_index('delivery_date', inplace=True)

    # Perform seasonal decomposition
    result = seasonal_decompose(df_service['cnt_work_order'], model='additive', period=13)

    # Create a tight layout for better visual clarity
    plt.tight_layout()

    # Plot the decomposed components using Seaborn
    fig, axes = plt.subplots(4, 1, figsize=(18, 6), sharex=True)
    sns.lineplot(data=result.observed, ax=axes[0], label='Observed', color='blue')
    sns.lineplot(data=result.trend, ax=axes[1], label='Trend', color='green')
    sns.lineplot(data=result.seasonal, ax=axes[2], label='Seasonal', color='red')
    sns.lineplot(data=result.resid, ax=axes[3], label='Residual', color='purple')

    for ax in axes:
        ax.legend()

    # Set a common title for the subplot
    plt.suptitle(f"Seasonal Decomposition - {service_desc}", y=1.02)

    # Display the plot
    plt.show()

    # Store results in the DataFrame
    service_results = pd.DataFrame({
        'service_desc': [service_desc] * len(result.trend),
        'delivery_date': result.trend.index,
        'trend': result.trend,
        'seasonal': result.seasonal,
        'residual': result.resid
    })
    decomposition_results = pd.concat([decomposition_results, service_results])

# Show the FacetGrid
plt.show()

# Display the DataFrame containing decomposition results
#print(decomposition_results.head())

"""## Step 4 : Different Forecasting

#### with Prophet
"""

from prophet.plot import plot_plotly, plot_components_plotly
from prophet import Prophet

"""###### Change Column Names for FB Prophet"""

service_desc_to_forecast = 'Standard Home delivery Truck'

# Filter DataFrame based on service_desc
df_service_1 = df_modified[df_modified['service_desc'] == service_desc_to_forecast].copy()

# Select relevant columns
df_service_1 = df_service_1[['delivery_date', 'cnt_work_order']]

# Rename columns for Prophet
df_service_1.columns = ['ds', 'y']

# Convert 'ds' to datetime type
df_service_1['ds'] = pd.to_datetime(df_service_1['ds'])

# Display the resulting DataFrame
print(df_service_1.head())

len(df_service_1)

"""###### Splitting the data for train and test"""

train_df, test_df = split_df_train_test(df_service_1, 'ds',0.8 )

# Assuming df_service_1 is my  DataFrame
split_percentage = 0.96

# Sort the DataFrame by 'ds'
df_service_1 = df_service_1.sort_values(by='ds')

# Calculate the split index
split_index = int(len(df_service_1) * split_percentage)

# Split the DataFrame into training and test sets
train_df = df_service_1[:split_index]
test_df = df_service_1[split_index:]

# Display the lengths of the training and test sets
print("Training set length:", len(train_df))
print("Test set length:", len(test_df))

"""##### Prediction Preparation"""

m = Prophet()
m.fit(train_df)
future = m.make_future_dataframe(periods=59) #MS for monthly, H for hourly
forecast = m.predict(future)

forecast.tail()

forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

test_df.tail()

"""##### Prophet model visualization"""

plot_plotly(m ,forecast)

plot_components_plotly(m, forecast)

prophet_df = pd.merge(test_df[['ds', 'y']], forecast[['ds', 'yhat']], on='ds', how='inner')
prophet_df.set_index('ds', inplace=True)
prophet_df.rename(columns={'y': 'actual', 'yhat': 'predicted'}, inplace=True)

len(prophet_df)

evaluate_forecast(method_name='Prophet', predictions=prophet_df['predicted'], actual_values=prophet_df['actual'])



plot_actual_vs_predicted(prophet_df)

"""#### with LSTM"""

service_desc_to_forecast = 'Standard Home delivery Truck'
# Read the entire DataFrame
df_lstm_all = pd.read_csv('/content/drive/MyDrive/TimeSeries/delivery_trasactions_service_balanced.csv',
                       parse_dates=True)

# Filter rows where service_desc is 'Standard Home delivery Truck'
df_lstm_service_1 = df_lstm_all[df_lstm_all['service_desc'] == service_desc_to_forecast ][['delivery_date', 'cnt_work_order']].copy()

df_lstm_service_1.dtypes

"""### How the data looks like from distribution of values"""

df_lstm_service_1.describe()

"""##### Test and Training set using train test split"""

train , test = split_df_train_test(df_lstm_service_1, 'delivery_date',0.96 )

"""###### Scale the data
###### data clearly has outliers and a standard scaler will not work
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
# 'cnt_work_order' is the target variable
scaler.fit(train)
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)

from keras.preprocessing.sequence import TimeseriesGenerator

"""##### Define Generator"""

n_input = 7
n_features = 1
n_batch_size = 32
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=n_batch_size)

X,y = generator[0]
print(f'Given the Array: \n{X.flatten()}')
print(f'Predict this y: \n {y}')

"""##### batch size, input,feature"""

X.shape

"""##### The NN model for LSTM"""

import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(128, input_shape= (n_input, n_features), return_sequences=True))
model.add(tf.keras.layers.LeakyReLU(alpha=0.5))
model.add(tf.keras.layers.LSTM(128, return_sequences=True))
model.add(tf.keras.layers.LeakyReLU(alpha=0.5))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.LSTM(64, return_sequences=False))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(1))

model.summary()

"""#### Fitting the model"""

early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=2,
                                                    mode='min')

model.compile(loss=tf.losses.MeanSquaredError(),
              optimizer=tf.optimizers.Adam(),
              metrics=[tf.metrics.MeanAbsoluteError()])

history = model.fit(generator, epochs=50,
                    shuffle=False,
                    callbacks=[early_stopping])

loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)),loss_per_epoch)

last_train_batch = scaled_train[-7:]

#n_input = 7
#n_features = 1
#n_batch_size = 32
last_train_batch = last_train_batch.reshape((1, n_input, n_features))

model.predict(last_train_batch)

scaled_test[0]

test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

for i in range(len(test)):

    # get the prediction value for the first batch
    current_pred = model.predict(current_batch)[0]

    # append the prediction into the array
    test_predictions.append(current_pred)

    # use the prediction to update the batch and remove the first value
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

test_predictions

test.head()

lstm_predictions = scaler.inverse_transform(test_predictions)

test['Predicted'] = lstm_predictions

plot_actual_vs_predicted(test, actual_column='cnt_work_order', predicted_column='Predicted')

evaluate_forecast(method_name='LSTM', predictions=test['Predicted'], actual_values=test['cnt_work_order'])

"""#### with SARIMA"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

df_sarimax_all =pd.DataFrame()
# Load pre processed data
df_sarima_all = pd.read_csv('/content/drive/MyDrive/TimeSeries/delivery_trasactions_service_balanced.csv')
service_desc_to_forecast = 'Standard Home delivery Truck'
# Filter rows where service_desc is 'Standard Home delivery Truck'
df_sarima_service_1 = df_sarima_all[df_sarima_all['service_desc'] == service_desc_to_forecast][['cnt_work_order']].copy()
# Convert 'delivery_date' to datetime format
df_sarimax_service_1['delivery_date'] = pd.to_datetime(df_sarimax_service_1['delivery_date'])

# Sort the DataFrame by 'delivery_date'
df_sarimax_service_1.sort_values(by='delivery_date', inplace=True)

# Set 'delivery_date' as the DataFrame index
df_sarimax_service_1.set_index('delivery_date', inplace=True)

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(df_sarimax_service_1['delivery_date'], df_sarimax_service_1['cnt_work_order'], label='Actual')
plt.title('Actual Data')
plt.xlabel('Date')
plt.ylabel('Count of Work Orders')
plt.legend()
plt.show()

# SARIMAX Model
order = (1, 0, 1)  # p, d, q
seasonal_order = (1, 0, 1, 12)  # P, D, Q, s

"""########### : The MLE is an iterative optimization algorithm used to find the parameter values that maximize the likelihood function, which measures how well the model explains the observed data. If the algorithm is unable to find the optimal parameters, it might indicate that the model is not well-suited for the data or that the initial parameter values provided are not appropriate."""

# Fit the SARIMAX model
model = SARIMAX(df_sarimax_service_1['cnt_work_order'], order=order, seasonal_order=seasonal_order)
results = model.fit(disp=False, maxiter=1000)  # Set disp=False to suppress convergence warnings

# Get predictions
forecast_start_date = df_sarimax_service_1.index[-1] + pd.Timedelta(days=1)
print('Forecast Start Date',forecast_start_date)
forecast_end_date = forecast_start_date + pd.Timedelta(days=30)
print('Forecast End Date',forecast_end_date)
forecast_index = pd.date_range(start=forecast_start_date, end=forecast_end_date, freq='D')

# Forecast
forecast_values = results.forecast(steps=len(forecast_index))

# Plot forecasted values
plt.figure(figsize=(12, 6))
plt.plot(df_sarimax_service_1.index, df_sarimax_service_1['cnt_work_order'], label='Actual')
plt.plot(forecast_index, forecast_values, label='Forecast', linestyle='dashed')
plt.title('Actual vs Forecasted Data')
plt.xlabel('Date')
plt.ylabel('Count of Work Orders')
plt.legend()
plt.show()

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(df_sarimax_service_1['cnt_work_order'], label='Actual', marker='o')
plt.plot(predicted_means_plot, label='Predicted', linestyle='--', color='red', marker='o')
plt.title('Actual vs Predicted Values')
plt.xlabel('Date')
plt.ylabel('cnt_work_order')
plt.xlim(pd.to_datetime(start_date_plot), pd.to_datetime(end_date_plot))  # Set x-axis limits
plt.legend()
plt.show()

# Get predictions
predictions = results.get_prediction(start=pd.to_datetime(start_date),
                                     end=pd.to_datetime(end_date))
predicted_means = predictions.predicted_mean

# Filter true_values and predicted_means to the specified date range
true_values_plot = df_sarimax_service_1['cnt_work_order'][start_date_plot:end_date_plot]
predicted_means_plot = predicted_means[start_date_plot:end_date_plot]

# Calculate MAE, MSE, RMSE, MAPE for the specified date range
mae_plot = mean_absolute_error(true_values_plot, predicted_means_plot)
mse_plot = mean_squared_error(true_values_plot, predicted_means_plot)
rmse_plot = np.sqrt(mse_plot)
mape_plot = np.mean(np.abs((true_values_plot - predicted_means_plot) / true_values_plot)) * 100

# Print the results for the specified date range
print(f'MAE for August 2023: {mae_plot}')
print(f'MSE for August 2023: {mse_plot}')
print(f'RMSE for August 2023: {rmse_plot}')
print(f'MAPE for August 2023: {mape_plot}')